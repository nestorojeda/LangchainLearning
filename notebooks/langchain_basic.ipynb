{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This loads variables from .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your setup by running a simple script that prints out the OpenAI API key (without exposing it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your API Key is set: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Your API Key is set:\", bool(os.getenv(\"OPENAI_API_KEY\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a basic chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversed Text: niahCgnaL olleH\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import TransformChain\n",
    "\n",
    "# Define a simple function that reverses a string as an example.\n",
    "def reverse_text(text: str) -> str:\n",
    "    return text[::-1]\n",
    "\n",
    "# Create a transform function\n",
    "def transform_func(inputs):\n",
    "    text = inputs[\"text\"]\n",
    "    return {\"output\": reverse_text(text)}\n",
    "\n",
    "# Create a chain that uses this function\n",
    "class ReverseChain(TransformChain):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            input_variables=[\"text\"],\n",
    "            output_variables=[\"output\"],\n",
    "            transform=transform_func\n",
    "        )\n",
    "\n",
    "# Instantiate and test the chain\n",
    "chain = ReverseChain()\n",
    "result = chain({\"text\": \"Hello LangChain\"})\n",
    "print(\"Reversed Text:\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: Okay, here's a fun fact about LangChain:\n",
      "\n",
      "**LangChain was initially conceived as a way to make it easier to build \"conversational AI agents\" â€“ essentially, chatbots that could actually *reason* and *remember* things across multiple turns of a conversation.**\n",
      "\n",
      "Initially, the project started as a simple library for connecting LLMs (Large Language Models) to data sources. However, the team quickly realized the *real* power lay in building agents that could chain together different LLM calls, tools, and memory to create truly interactive and intelligent conversational experiences. \n",
      "\n",
      "**The \"Chain\" in LangChain refers to this ability to link together different components to create complex workflows.**\n",
      "\n",
      "Pretty cool, right? \n",
      "\n",
      "---\n",
      "\n",
      "Would you like to know more about a specific aspect of LangChain, like its components or use cases?\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# Create an Ollama LLM instance\n",
    "llm = OllamaLLM(model=\"gemma3:4b\")\n",
    "\n",
    "# Define a simple prompt and get a completion\n",
    "prompt = \"Tell me a fun fact about LangChain.\"\n",
    "response = llm.invoke(prompt)\n",
    "print(\"LLM Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt: Translate the following English text to French: How are you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template with variables\n",
    "template = \"Translate the following English text to French: {text}\"\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "\n",
    "# Format the prompt with your input text\n",
    "formatted_prompt = prompt.format(text=\"How are you today?\")\n",
    "print(\"Formatted Prompt:\", formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi, who are you?\n",
      "Assistant: Hi there! I'm Gemma, a large language model created by the Gemma team at Google DeepMind. Iâ€™m an open-weights model, which means Iâ€™m widely available for public use! \n",
      "\n",
      "Itâ€™s nice to meet you. ðŸ˜Š \n",
      "\n",
      "What can I do for you today?\n",
      "User: Can you remind me what we talked about?\n",
      "Assistant: Please provide me with a little more context! I have no memory of our previous conversations unless you give me a starting point. ðŸ˜Š \n",
      "\n",
      "To help me remind you, could you tell me:\n",
      "\n",
      "*   **What were we talking about?** (e.g., \"We were discussing travel plans,\" or \"We were brainstorming ideas for a story.\")\n",
      "*   **When did we last talk?** (e.g., \"Just a few minutes ago,\" or \"Yesterday afternoon.\")\n",
      "\n",
      "The more information you give me, the better I can help you!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# Create a simple chain with the prompt template and LLM\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | llm\n",
    "\n",
    "# Create the runnable with message history\n",
    "conversation_with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: ChatMessageHistory(),  # Function to get history for a session\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# Simulate a conversation with a session ID\n",
    "session_id = \"example_session\"\n",
    "\n",
    "# First interaction\n",
    "print(\"User: Hi, who are you?\")\n",
    "response = conversation_with_memory.invoke(\n",
    "    {\"input\": \"Hi, who are you?\"}, \n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Assistant:\", response)\n",
    "\n",
    "# Second interaction that references the history\n",
    "print(\"User: Can you remind me what we talked about?\")\n",
    "response = conversation_with_memory.invoke(\n",
    "    {\"input\": \"Can you remind me what we talked about?\"}, \n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agents & Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: this implementation is not the most modern one, but it is the most simple one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to calculate 15 multiplied by 3.\n",
      "Action: Calculator\n",
      "Action Input: 15 * 3\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m45\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 45\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Agent Response: 45\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "# Define a simple calculator tool\n",
    "def calculator_tool(query: str) -> str:\n",
    "    try:\n",
    "        result = eval(query)  # Caution: eval() should only be used with trusted input.\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "calculator = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=calculator_tool,\n",
    "    description=\"Performs basic arithmetic calculations.\"\n",
    ")\n",
    "\n",
    "# Initialize an agent with the tool\n",
    "agent = initialize_agent([calculator], llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "# Test the agent with a math query\n",
    "query = \"What is 15 * 3?\"\n",
    "result = agent.run(query)\n",
    "print(\"Agent Response:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nojeda/repo/LangchainLearning/.venv/lib/python3.13/site-packages/langchain/indexes/vectorstore.py:171: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 2083.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: Heisenbergâ€™s insight,\n",
      "Limits what we know of all,\n",
      "Quantum world unfolds.\n",
      "Final Answer: Here's a haiku based on the provided text about the Uncertainty Principle:\n",
      "\n",
      "Heisenbergâ€™s insight,\n",
      "Limits what we know of space,\n",
      "Time and place entwined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.embeddings import OllamaEmbeddings  # or another embedding model\n",
    "from langchain_ollama.llms import OllamaLLM  # assuming you're using Ollama\n",
    "\n",
    "# Create an embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"all-minilm\")  # or another suitable model\n",
    "llm = OllamaLLM(model=\"gemma3:4b\")  # or another suitable model\n",
    "\n",
    "# Create a custom loader class to handle potential errors\n",
    "class ErrorHandlingTextLoader(TextLoader):\n",
    "    def load(self):\n",
    "        try:\n",
    "            return super().load()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.file_path}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "# Load all text files from a directory\n",
    "loader = DirectoryLoader(\n",
    "    \".././docs/\", \n",
    "    glob=\"**/*.txt\",  # This pattern matches all .txt files, including in subdirectories\n",
    "    loader_cls=ErrorHandlingTextLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "# Create the index with the embedding model\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=embeddings,\n",
    "    vectorstore_kwargs={\"collection_name\": \"langchain_docs\"},\n",
    ").from_loaders([loader])\n",
    "\n",
    "\n",
    "# Query the index and get a response\n",
    "query = \"Make a haiku about who discovered the Uncertainty Principle\"\n",
    "docs = index.query(query, llm=llm)  # Note: Using query method instead of search\n",
    "print(\"Retrieved Documents:\", docs)\n",
    "\n",
    "# If you want to manually process the docs and send to LLM\n",
    "docs = index.vectorstore.similarity_search(query)\n",
    "combined_text = \" \".join([doc.page_content for doc in docs])\n",
    "final_prompt = f\"Based on the following context, answer the query: {combined_text}\\n\\nQuery: {query}\"\n",
    "response = llm.invoke(final_prompt)  # Using invoke() instead of direct call\n",
    "print(\"Final Answer:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
